@inproceedings{Garrett1986,
address = {New York, New York, USA},
author = {Garrett, L. Nancy and Smith, Karen E.},
booktitle = {Conference proceedings on Object-oriented programming systems, languages and applications - OOPLSA '86},
doi = {10.1145/28697.28717},
file = {:Volumes/GoogleDrive/My Drive/Thesis/Literature/garrett1986.pdf:pdf},
isbn = {0897912047},
pages = {202--213},
publisher = {ACM Press},
title = {{Building a timeline editor from prefab parts: the architecture of an object-oriented application}},
url = {http://portal.acm.org/citation.cfm?doid=28697.28717},
year = {1986}
}
@article{Griswold2006,
abstract = {Aspect-oriented programming languages such as AspectJ offer new mechanisms for decomposing systems into modules and composing modules into systems. Common ways of using these mechanisms couple aspects to complex, changeable implementation details, which can compromise modularity. The crosscut programming interface (XPI) can significantly improve modularity in the design of programs employing AspectJ-style AOP. The use of XPIs doesn't limit the use of existing AOP mechanisms or require new ones, and the approach appears to generalize to other languages.This article is part of a special issue on aspect-oriented programming. {\textcopyright} 2006 IEEE.},
author = {Griswold, William G. and Shonle, Macneil and Sullivan, Kevin and Song, Yuanyuan and Tewari, Nishit and Cai, Yuanfang and Rajan, Hridesh},
doi = {10.1109/MS.2006.24},
file = {:Volumes/GoogleDrive/My Drive/Thesis/Literature/griswold2006.pdf:pdf},
issn = {07407459},
journal = {IEEE Software},
number = {1},
pages = {51--60},
title = {{Modular software design with crosscutting interfaces}},
volume = {23},
year = {2006}
}
@article{DeVries1997,
abstract = {Conversion to client/server systems from host-centric systems can be accomplished with careful attention to planning and development procedures. A key component in a successful conversion is a customized definition of client/server which complements the organization's mission. Another key component is the development of an infrastructure which includes hardware, software and people. The training of users and design personnel is crucial to successful conversion. Discusses definitions, strategies, hardware, software, and the pitfalls to be avoided.},
author = {{De Vries}, Wietze A. and Fleck, Robert A.},
doi = {10.1108/02635579710176795},
file = {:Volumes/GoogleDrive/My Drive/Thesis/Literature/devries1997.pdf:pdf},
issn = {02635577},
journal = {Industrial Management and Data Systems},
number = {6},
pages = {222--232},
title = {{Client/ server infrastructure: A case study in planning and conversion}},
volume = {97},
year = {1997}
}
@article{Andreadis2018,
abstract = {Datacenters act as cloud-infrastructure to stakeholders across industry, government, and academia. To meet growing demand yet operate efficiently, datacenter operators employ increasingly more sophisticated scheduling systems, mechanisms, and policies. Although many scheduling techniques already exist, relatively little research has gone into the abstraction of the scheduling process itself, hampering design, tuning, and comparison of existing techniques. In this work, we propose a reference architecture for datacenter schedulers. The architecture follows five design principles: components with clearly distinct responsibilities, grouping of related components where possible, separation of mechanism from policy, scheduling as complex workflow, and hierarchical multi-scheduler structure. To demonstrate the validity of the reference architecture, we map to it state-of-the-art datacenter schedulers. We find scheduler-stages are commonly underspecified in peer-reviewed publications. Through trace-based simulation and real-world experiments, we show underspecification of scheduler-stages can lead to significant variations in performance.},
author = {Andreadis, Georgios and Versluis, Laurens and Mastenbroek, Fabian and Iosup, Alexandru},
doi = {10.1109/SC.2018.00040},
file = {:Volumes/GoogleDrive/My Drive/Thesis/Literature/andreadis2018.pdf:pdf},
isbn = {9781538683842},
journal = {Proceedings - International Conference for High Performance Computing, Networking, Storage, and Analysis, SC 2018},
keywords = {Datacenter,Reference Architecture,Scheduling},
pages = {478--492},
title = {{A reference architecture for datacenter scheduling: Design, validation, and experiments}},
year = {2018}
}
@article{Abate2013,
abstract = {Context: The success of modern software distributions in the Free and Open Source world can be explained, among other factors, by the availability of a large collection of software packages and the possibility to easily install and remove those components using state-of-the-art package managers. However, package managers are often built using a monolithic architecture and hard-wired and ad-hoc dependency solvers implementing some customized heuristics. Objective: We aim at laying the foundation for improving on existing package managers. Package managers should be complete, that is find a solution whenever there exists one, and allow the user to specify complex criteria that define how to pick the best solution according to the user's preferences. Method: In this paper we propose a modular architecture relying on precise interface formalisms that allows the system administrator to choose from a variety of dependency solvers and backends. Results: We have built a working prototype-called MPM-following the design advocated in this paper, and we show how it largely outperforms a variety of current package managers. Conclusion: We argue that a modular architecture, allowing for delegating the task of constraint solving to external solvers, is the path that leads to the next generation of package managers that will deliver better results, offer more expressive preference languages, and be easily adaptable to new platforms. {\textcopyright} 2012 Elsevier B.V. All rights reserved.},
author = {Abate, Pietro and {Di Cosmo}, Roberto and Treinen, Ralf and Zacchiroli, Stefano},
doi = {10.1016/j.infsof.2012.09.002},
file = {:Volumes/GoogleDrive/My Drive/Thesis/Literature/abate2013.pdf:pdf},
issn = {09505849},
journal = {Information and Software Technology},
keywords = {Open source,Package manager,Software components,Software dependencies,Software repositories},
number = {2},
pages = {459--474},
publisher = {Elsevier B.V.},
title = {{A modular package manager architecture}},
url = {http://dx.doi.org/10.1016/j.infsof.2012.09.002},
volume = {55},
year = {2013}
}
@article{Casanova1991,
author = {Casanova, M A and Scientific, Rio and Furtado, A L and Tucherman, L},
file = {:Volumes/GoogleDrive/My Drive/Thesis/Literature/casanova1991.pdf:pdf},
journal = {Database},
keywords = {abstract data types,and phrases,consistency preservation},
number = {June},
pages = {209--234},
title = {{Tool for Modular Database Design}},
volume = {16},
year = {1991}
}
@article{Tang2007,
abstract = {Currently, Internet hosting centers and content distribution networks leverage statistical multiplexing to meet the performance requirements of a number of competing hosted network services. Developing efficient resource allocation mechanisms for such services requires an understanding of both the short-term and long-term behavior of client access patterns to these competing services. At the same time, streaming media services are becoming increasingly popular, presenting new challenges for designers of shared hosting services. These new challenges result from fundamentally new characteristics of streaming media relative to traditional web objects, principally different client access patterns and significantly larger computational and bandwidth overhead associated with a streaming request. To understand the characteristics of these new workloads we use two long-term traces of streaming media services to develop MediSyn, a publicly available streaming media workload generator. In summary, this paper makes the following contributions: (i) we propose a framework for modeling long-term behavior of network services by capturing the process of file introduction, non-stationary popularity of media accesses, file duration, encoding bit rate, and session duration. (ii) We propose a variety of practical models based on the study of the two workloads. (iii) We develop an open-source synthetic streaming service workload generator to demonstrate the capability of our framework to capture the models. {\textcopyright} 2006 Elsevier B.V. All rights reserved.},
author = {Tang, Wenting and Fu, Yun and Cherkasova, Ludmila and Vahdat, Amin},
doi = {10.1016/j.comnet.2006.05.003},
file = {:Volumes/GoogleDrive/My Drive/Thesis/Literature/tang2007.pdf:pdf},
issn = {13891286},
journal = {Computer Networks},
keywords = {File life span,Media access patterns,Modeling,Non-stationary popularity,Streaming media server workload,Synthetic workload generator,Temporal and static properties,Zipf-Mandelbrot law},
number = {1},
pages = {336--356},
title = {{Modeling and generating realistic streaming media server workloads}},
volume = {51},
year = {2007}
}
@article{Mourad1996,
abstract = {We examine issues related to the design of a storage server for video-on-demand (VOD) applications. The storage medium considered is magnetic disks or arrays of disks. We investigate disk scheduling policies, buffer management policies and I/O bus protocol issues. We derive the number of sessions that can be supported from a single disk or an array of disks and determine the amount of buffering required to support a given number of users. Furthermore. we propose a scheduling mechanism for disk accesses that significantly lowers the buffer-size requirements in the case of disk arrays. The buffer size required under the proposed scheme is independent of the number of disks in the array. This property allows for striping video content over a large number of disks to achieve higher concurrency in access to a particular video object. This enables the server to satisfy hundreds of independent requests to the same video object or to hundreds of different objects while storing only one copy of each video object. The reliability implications of striping content over a large number of disks are addressed and two solutions are proposed. Finally, we examine various policies for dealing with disk thermal calibration and the placement of videos on disks and disk arrays.},
author = {Mourad, Antoine N.},
doi = {10.1007/s005300050013},
file = {:Volumes/GoogleDrive/My Drive/Thesis/Literature/mourad1996.pdf:pdf},
issn = {09424962},
journal = {Multimedia Systems},
keywords = {Disk striping,Multimedia,RAID,SCSI-MPEG,Video-on-demand},
number = {2},
pages = {70--86},
title = {{Issues in the design of a storage server for video-on-demand}},
volume = {4},
year = {1996}
}
@article{Bauer2003,
abstract = {We present a new module system for Java that improves upon many of the deficiencies of the Java package system and gives the programmer more control over dynamic linking. Our module system provides explicit interfaces, multiple views of modules based on hierarchical nesting and more flexible name-space management than the Java package system. Relationships between modules are explicitly specified in module description files. We provide more control over dynamic linking by allowing import statements in module description files to require that imported modules be annotated with certain properties, which we implement by digital signatures. Our module system is compatible enough with standard Java to be implemented as a source-to-source and bytecode-to-bytecode transformation wrapped around a standard Java compiler, using a standard Java virtual machine (JVM).},
author = {Bauer, Lujo and Appel, Andrew W. and Felten, Edward W.},
doi = {10.1002/spe.516},
file = {:Volumes/GoogleDrive/My Drive/Thesis/Literature/bauer2003.pdf:pdf},
issn = {00380644},
journal = {Software - Practice and Experience},
keywords = {Abstract data types,Computer security,Dynamic linking,Hierarchical module system,Information hiding,Java,Name-space management},
number = {5},
pages = {461--480},
title = {{Mechanisms for secure modular programming in Java}},
volume = {33},
year = {2003}
}
@article{Lee2005,
abstract = {Large scale interactive online games aim to support a very large number of game players simultaneously. To support hundreds of thousands of concurrent players, game providers have so far focused on developing highly scalable game server architectures and extensible network infrastructures. Recently, distributed online games are beginning to incorporate more interactive features and action sequences; thus, it becomes increasingly important to provision server resources in an efficient manner to support real-time interaction between the users. In this paper, we present a novel distributed algorithm to select game servers for a group of clients participating in a large scale interactive online game session. The goal of server selection is to minimize the server resource usage while satisfying a real-time delay constraint. We develop a synchronization delay model for interactive games and formulate the server selection problem, and prove that the considered problem is NP-hard. The proposed algorithm, called zoom-in-zoom-out, is adaptive to session dynamics (e.g., clients join) and lets the clients select appropriate servers in a distributed manner such that the server resource is efficiently utilized. Using simulation, we study the performance of the proposed algorithm and show that it is simple, yet effective in achieving its design goal. In particular, we show that the performance of our algorithm is comparable to, or sometimes even better than, that of centralized greedy algorithms, which require global information and extensive computations. {\textcopyright} 2005 Published by Elsevier B.V.},
author = {Lee, Kang Won and Ko, Bong Jun and Calo, Seraphin},
doi = {10.1016/j.comnet.2005.04.006},
file = {:Volumes/GoogleDrive/My Drive/Thesis/Literature/lee2005.pdf:pdf},
issn = {13891286},
journal = {Computer Networks},
keywords = {Distributed Algorithm,Distributed systems,MMOG,On-line game infrastructure,Resource allocation,Synchronization delay model},
number = {1},
pages = {84--102},
title = {{Adaptive server selection for large scale interactive online games}},
volume = {49},
year = {2005}
}
@article{Jang2011,
abstract = {Increasing energy consumption in server consolidation environments leads to high maintenance costs for data centers. Main memory, no less than processor, is a major energy consumer in this environment. This paper proposes a technique for reducing memory energy consumption using virtual machine scheduling in multicore systems. We devise several heuristic scheduling algorithms by using a memory power simulator, which we designed and implemented. We also implement the biggest cover set first (BCSF) scheduling algorithm in the working server system. Through extensive simulation and implementation experiments, we observe the effectiveness of the memory-aware virtual machine scheduling in saving memory energy. In addition, we find out that power-aware memory management is essential to reduce the memory energy consumption. {\textcopyright} 2011 IEEE.},
author = {Jang, Jae Wan and Jeon, Myeongjae and Kim, Hyo Sil and Jo, Heeseung and Kim, Jin Soo and Maeng, Seungryoul},
doi = {10.1109/TC.2010.82},
file = {:Volumes/GoogleDrive/My Drive/Thesis/Literature/jang2011.pdf:pdf},
issn = {00189340},
journal = {IEEE Transactions on Computers},
keywords = {DRAM energy,multicore processor,scheduling,virtual machine},
number = {4},
pages = {552--564},
title = {{Energy reduction in consolidated servers through memory-aware virtual machine scheduling}},
volume = {60},
year = {2011}
}
@article{Kunkel2000,
author = {Kunkel, S. R. and Eickemeyer, R. J. and Lipasti, M. H. and Mullins, T. J. and O'Krafka, B. and Rosenberg, H. and Vanderwiel, S. P. and Vitale, P. L. and Whitley, L. D.},
doi = {10.1147/rd.446.0851},
file = {:Volumes/GoogleDrive/My Drive/Thesis/Literature/kunkel2000.pdf:pdf},
issn = {00188646},
journal = {IBM Journal of Research and Development},
number = {6},
pages = {851--872},
title = {{A performance methodology for commercial servers}},
volume = {44},
year = {2000}
}
@article{Nanda2000,
abstract = {Modern system design often requires multiple levels of simulation for design validation and performance debugging. However, while machines have gotten faster, and simulators have become more detailed, simulation speeds have not tracked machine speeds. As a result, it is difficult to simulate realistic problem sizes and hardware configurations for a target machine. Instead, researchers have focussed on developing scaling methodologies and running smaller problem sizes and configurations that attempt to represent the behavior of the real problem. Given the increasing size of problems today, it is unclear whether such an approach yields accurate results. Moreover, although commercial workloads are prevalent and important in today's marketplace, many simulation tools are unable to adequately profile such applications, let alone for realistic sizes. In this paper we present a hardware-based emulation tool that can be used to aid memory system designers. Our focus is on the memory system because the ever-widening gap between processor and memory speeds means that optimizing the memory subsystem is critical for performance. We present the design of the Memory Instrumentation and Emulation System (MemorIES). MemorIES is a programmable tool designed using FPGAs and SDRAMs. It plugs into an SMP bus to perform on-line emulation of several cache configurations, structures and protocols while the system is running real-life workloads in real-time, without any slowdown in application execution speed. We demonstrate its usefulness in several case studies, and find several important results. First, using traces to perform system evaluation can lead to incorrect results (off by 100% or more in some cases) if the trace size is not sufficiently large. Second, MemorIES is able to detect performance problems by profiling miss behavior over the entire course of a run, rather than relying on a small interval of time. Finally, we observe that previous studies of SPLASH2 applications using scaled application sizes can result in optimistic miss rates relative to real sizes on real machines, providing potentially misleading data when used for design evaluation. {\textcopyright} 2000 ACM.},
author = {Nanda, Ashwini and Mak, Kwok Ken and Sugavanam, Krishnan and Sahoo, Ramendra K. and Soundararajan, Vijayaraghavan and Smith, T. Basil},
doi = {10.1145/356989.356993},
file = {:Volumes/GoogleDrive/My Drive/Thesis/Literature/nanda2000.pdf:pdf},
isbn = {1581133170},
issn = {03621340},
journal = {SIGPLAN Notices (ACM Special Interest Group on Programming Languages)},
number = {11},
pages = {37--48},
title = {{MemorIES: A programmable, real-time hardware emulation tool for multiprocessor server design}},
volume = {35},
year = {2000}
}
@article{Wang2015,
abstract = {Data center network (DCN) architecture is regarded as one of the most important determinants of network performance. As the most typical representatives of DCN architecture designs, the server-centric scheme stands out due to its good performance in various aspects. In this paper, we firstly present the design, implementation and evaluation of SprintNet, a novel server-centric network architecture for data centers. SprintNet achieves high performance in network capacity, fault tolerance, and network latency. SprintNet is also a scalable, yet low-diameter network architecture where the maximum shortest distance between any pair of servers can be limited by no more than four and is independent of the number of layers. The specially designed routing schemes for SprintNet strengthen its merits. However, all of these kind of server-centric architectures still suffer from some critical shortcomings owing to the server's responsibility of forwarding packets. With regard to these issues, in this paper, we then propose a hardware based approach, named "Forwarding Unit" to provide an effective solution to these drawbacks and improve the efficiency of server-centric architectures. Both theoretical analysis and simulations are conducted to evaluate the overall performance of SprintNet and the Forwarding Unit approach with respect to cost-effectiveness, fault-tolerance, system latency, packet loss ratio, aggregate bottleneck throughput, and average path length. The evaluation results convince the feasibility and good performance of both SprintNet and Forwarding Unit.},
author = {Wang, Ting and Su, Zhiyang and Xia, Yu and Muppala, Jogesh and Hamdi, Mounir},
doi = {10.1016/j.comnet.2015.01.006},
file = {:Volumes/GoogleDrive/My Drive/Thesis/Literature/wang2015.pdf:pdf},
issn = {13891286},
journal = {Computer Networks},
keywords = {Data center network,Flat network,Interconnection architecture,Network topology,Server-centric},
pages = {283--296},
publisher = {Elsevier B.V.},
title = {{Designing efficient high performance server-centric data center network architecture}},
url = {http://dx.doi.org/10.1016/j.comnet.2015.01.006},
volume = {79},
year = {2015}
}
@article{Felter2003,
author = {Felter, Wesley M. and Keller, Tom W. and Kistler, Michael D. and Lefurgy, Charles and Rajamani, Karthick and Rajamony, Ram and Rawson, Freeman L. and Smith, Bruce A. and Hensbergen, Eric Van},
doi = {10.1147/rd.475.0671},
file = {:Volumes/GoogleDrive/My Drive/Thesis/Literature/felter2003.pdf:pdf},
issn = {00188646},
journal = {IBM Journal of Research and Development},
number = {5-6},
pages = {671--688},
title = {{On the performance and use of dense servers}},
volume = {47},
year = {2003}
}
@article{Gandhi2010,
abstract = {In this paper we consider server farms with a setup cost. This model is common in manufacturing systems and data centers, where there is a cost to turn servers on. Setup costs always take the form of a time delay, and sometimes there is additionally a power penalty, as in the case of data centers. Any server can be either on, off, or in setup mode. While prior work has analyzed single servers with setup costs, no analytical results are known for multi-server systems. In this paper, we derive the first closed-form solutions and approximations for the mean response time and mean power consumption in server farms with setup costs. We also analyze variants of server farms with setup, such as server farm models with staggered boot up of servers, where at most one server can be in setup mode at a time, or server farms with an infinite number of servers. For some variants, we find that the distribution of response time can be decomposed into the sum of response time for a server farm without setup and the setup time. Finally, we apply our analysis to data centers, where both response time and power consumption are key metrics. Here we analyze policy design questions such as whether it pays to turn servers off when they are idle, whether staggered boot up helps, how to optimally mix policies, and other questions related to the optimal data center size. {\textcopyright} 2010 Elsevier B.V. All rights reserved.},
author = {Gandhi, Anshul and Harchol-Balter, Mor and Adan, Ivo},
doi = {10.1016/j.peva.2010.07.004},
file = {:Volumes/GoogleDrive/My Drive/Thesis/Literature/gandhi2010.pdf:pdf},
issn = {01665316},
journal = {Performance Evaluation},
keywords = {Data centers,M/M/k,Power management,Setup times,Staggered boot up},
number = {11},
pages = {1123--1138},
publisher = {Elsevier B.V.},
title = {{Server farms with setup costs}},
url = {http://dx.doi.org/10.1016/j.peva.2010.07.004},
volume = {67},
year = {2010}
}
@article{Barroso2007,
abstract = {Energy-proportional designs would enable large energy savings in servers, potentially doubling their efficiency in real-life use. Achieving energy-proportionality will require significant improvements in the energy usage profile of every system component, particularly the memory and disk subsystems. {\textcopyright} 2007 IEEE.},
author = {Barroso, Luiz Andr{\'{e}} and H{\"{o}}lzle, Urs},
doi = {10.1109/MC.2007.443},
file = {:Volumes/GoogleDrive/My Drive/Thesis/Literature/barroso2007.pdf:pdf},
issn = {00189162},
journal = {Computer},
keywords = {Energy-proportional computing,Green computing},
number = {12},
pages = {33--37},
title = {{The case for energy-proportional computing}},
volume = {40},
year = {2007}
}
@article{Guo2009,
author = {Guo, Chuanxiong and Lu, Guohan and Li, Dan and Wu, Haitao and Zhang, Xuan and Shi, Yunfeng and Tian, Chen and Zhang, Yongguang and Lu, Songwu},
doi = {978-1-60558-594-9/09/08},
file = {:Volumes/GoogleDrive/My Drive/Thesis/Literature/guo2009.pdf:pdf},
isbn = {9781605585949},
journal = {Sigcomm'09},
keywords = {and chen tian were,interns and songwu lu,modular data center,multi-path,server-centric network,this work was performed,was a visiting,when xuan zhang,yunfeng shi},
pages = {63--74},
title = {{BCube : A High Performance , Server-centric Network Architecture for Modular Data Centers ∗ Categories and Subject Descriptors}},
year = {2009}
}
@article{Kozyrakis2010,
abstract = {The rapid growth of online services in the last decade has led to the development of large data centers to host these workloads. These large-scale online, user-facing services have unique engineering and capacity provisioning design requirements. The authors explore these requirements, focusing on system balancing, the impact of technology trends, and the challenges of online service workloads. {\textcopyright} 2006 IEEE.},
author = {Kozyrakis, Christos and Kansal, Aman and Sankar, Sriram and Vaid, Kushagra},
doi = {10.1109/MM.2010.73},
file = {:Volumes/GoogleDrive/My Drive/Thesis/Literature/kozyrakis2010.pdf:pdf},
issn = {02721732},
journal = {IEEE Micro},
keywords = {Scale-out design,Server balancing,data centers,hardware,large-scale production services,online services,server design,total cost of ownership},
number = {4},
pages = {8--19},
title = {{Server engineering insights for Large-Scale online services}},
volume = {30},
year = {2010}
}
@article{Neufeldt2016,
abstract = {A fundamental challenge in data center networking is how to efficiently interconnect an exponentially increasing number of servers. This paper presents DCell, a novel network structure that has many desirable features for data center networking. DCell is a recursively defined structure, in which a high-level DCell is constructed from many low-level DCells and DCells at the same level are fully connected with one another. DCell scales doubly exponentially as the node degree increases. DCell is fault tolerant since it does not have single point of failure and its distributed fault-tolerant routing protocol performs near shortest-path routing even in the presence of severe link or node failures. DCell also provides higher network capacity than the traditional tree-based structure for various types of services. Furthermore, DCell can be incrementally expanded and a partial DCell provides the same appealing features. Results from theoret- ical analysis, simulations, and experiments show that DCell is a viable interconnection structure for data centers.},
author = {Neufeldt, R},
file = {:Volumes/GoogleDrive/My Drive/Thesis/Literature/guo2008.pdf:pdf},
isbn = {9781605581750},
journal = {Sigcomm Ccr},
keywords = {data center,fault tolerance,network topology,throughput},
number = {4},
pages = {75--86},
title = {{Wicked Problems: Peacebuilding Evaluation Ethics, Determining What is Good and Right}},
url = {http://cdacollaborative.org/publication/wicked-problems-peacebuilding-evaluation-ethics-determining-good-right/},
volume = {38},
year = {2016}
}
@article{Menasce2000,
abstract = {Designing complex distributed client/server applications that meet performance requirements may prove extremely difficult in practice if software developers are not willing or do not have the time to help software performance analysts. This paper advocates the need to integrate both design and performance modeling activities so that one can help the other. We present a method developed and used by the authors in the design of a fairly large and complex client/server application. The method is based on a software performance engineering language developed by one of the authors. Use cases were developed and mapped to a performance modeling specification using the language. A compiler for the language generates an analytic performance model for the system. Service demand parameters at servers, storage boxes, and networks are derived by the compiler from the system specification. A detailed model of DBMS query optimizers allows the compiler to estimate the number of l/Os and CPU time for SQL statements. The paper concludes with some results of the application that prompted the development of the method and language. Index Terms-Software performance engineering, performance models, client/server systems, queuing networks, database query optimization, UML: , CLISSPE. {\textcopyright} 2000 IEEE.},
author = {Menasc{\'{e}}, Daniel A.},
doi = {10.1109/32.881718},
file = {:Volumes/GoogleDrive/My Drive/Thesis/Literature/gomaa2000.pdf:pdf},
issn = {00985589},
journal = {IEEE Transactions on Software Engineering},
number = {11},
pages = {1066--1085},
title = {{A Method for Design and Performance Modeling of Client/Server Systems}},
volume = {26},
year = {2000}
}
@article{Cherkasova2004,
abstract = {Understanding the nature of media server workloads is crucial to properly designing and provisioning current and future media services. The main issue we address in this paper is the workload analysis of today's enterprise media servers. This analysis aims to establish a set of properties specific to the enterprise media server workloads and to compare them to well-known related observations about the web server workloads. We partition the media workload properties in two groups: static and temporal. While the static properties provide more traditional and general characteristics of the underlying media fileset and quantitative properties of client accesses to those files (independent of the access time), the temporal properties reflect the dynamics and evolution of accesses to the media content over time. We propose two new metrics characterizing the temporal properties: 1) the new files impact metric characterizing the site evolution due to new content and 2) the life span metric reflecting the rates of change in accesses to the newly introduced files. We illustrate these new metrics with the analysis of two different enterprise media server workloads collected over a significant period of time. {\textcopyright} 2004 IEEE.},
author = {Cherkasova, Ludmila and Gupta, Minaxi},
doi = {10.1109/TNET.2004.836125},
file = {:Volumes/GoogleDrive/My Drive/Thesis/Literature/cherkasova2004.pdf:pdf},
issn = {10636692},
journal = {IEEE/ACM Transactions on Networking},
number = {5},
pages = {781--794},
title = {{Analysis of enterprise media server workloads: Access patterns, locality, content evolution, and rates of change}},
volume = {12},
year = {2004}
}
@article{Guo2013,
abstract = {A fundamental goal of data center networking is to efficiently interconnect a large number of servers with the low equipment cost. Several server-centric network structures for data centers have been proposed. They, however, are not truly expandable and suffer a low degree of regularity and symmetry. Inspired by the commodity servers in today's data centers that come with dual port, we consider how to build expandable and cost-effective structures without expensive high-end switches and additional hardware on servers except the two NIC ports. In this paper, two such network structures, called HCN and BCN, are designed, both of which are of server degree 2. We also develop the low overhead and robust routing mechanisms for HCN and BCN. Although the server degree is only 2, HCN can be expanded very easily to encompass hundreds of thousands servers with the low diameter and high bisection width. Additionally, HCN offers a high degree of regularity, scalability, and symmetry, which conform to the modular designs of data centers. BCN is the largest known network structure for data centers with the server degree 2 and network diameter 7. Furthermore, BCN has many attractive features, including the low diameter, high bisection width, large number of node-disjoint paths for the one-to-one traffic, and good fault-tolerant ability. Mathematical analysis and comprehensive simulations show that HCN and BCN possess excellent topological properties and are viable network structures for data centers. {\textcopyright} 1968-2012 IEEE.},
author = {Guo, Deke and Chen, Tao and Li, Dan and Li, Mo and Liu, Yunhao and Chen, Guihai},
doi = {10.1109/TC.2012.90},
file = {:Volumes/GoogleDrive/My Drive/Thesis/Literature/dekeguo2013.pdf:pdf},
issn = {00189340},
journal = {IEEE Transactions on Computers},
keywords = {Data center networking,interconnection networks,network structures},
number = {7},
pages = {1303--1317},
title = {{Expandable and cost-effective network structures for data centers using dual-port servers}},
volume = {62},
year = {2013}
}
@article{Mastenbroek2019,
author = {Mastenbroek, Fabian},
file = {:Volumes/GoogleDrive/My Drive/Thesis/Literature/Mastenbroek2019.pdf:pdf},
keywords = {cloud computing,datacenter,design space exploration,genetic algorithm,reference architecture,scheduling,simulation},
number = {Section 3},
title = {{A Systematic Design Space Exploration of Datacenter Schedulers}},
year = {2019}
}
@article{Sullivan2010,
abstract = {The emergence of aspect-oriented programming (AOP) languages has provided software designers with new mechanisms and strategies for decomposing programs into modules and composing modules into systems. What we do not yet fully understand is how best to use such mechanisms consistent with common modularization objectives such as the comprehensibility of programming code, its parallel development, dependability, and ease of change. The main contribution of this work is a new form of information-hiding interface for AOP that we call the crosscut programming interface, or XPI. XPIs abstract crosscutting behaviors and make these abstractions explicit. XPIs can be used, albeit with limited enforcement of interface rules, with existing AOP languages, such as AspectJ. To evaluate our notion of XPIs, we have applied our XPI-based design methodology. {\textcopyright} 2010 ACM.},
author = {Sullivan, Kevin and Griswold, William G. and Rajan, Hridesh and Song, Yuanyuan and Cai, Yuanfang and Shonle, MacNeil and Tewari, Nishit},
doi = {10.1145/1824760.1824762},
file = {:Volumes/GoogleDrive/My Drive/Thesis/Literature/sullivan2010.pdf:pdf},
issn = {1049331X},
journal = {ACM Transactions on Software Engineering and Methodology},
keywords = {Aspect-oriented programming,Design rules,Options},
number = {2},
pages = {1--42},
title = {{Modular aspect-oriented design with XPIs}},
volume = {20},
year = {2010}
}
@misc{DigitalOcean2020,
author = {DigitalOcean},
title = {{DigitalOcean Marketplace}},
url = {https://marketplace.digitalocean.com},
urldate = {2020-05-03},
year = {2020}
}
@article{Nierstrasz1992,
author = {Nierstrasz, Oscar and Gibbs, Simon and Tsichritzis, Dennis},
doi = {10.1145/130994.131005},
file = {:Volumes/GoogleDrive/My Drive/Thesis/Literature/Nierstrasz1992.pdf:pdf},
issn = {00010782},
journal = {Communications of the ACM},
month = {sep},
number = {9},
pages = {160--165},
title = {{Component-oriented software development}},
url = {http://portal.acm.org/citation.cfm?doid=130994.131005},
volume = {35},
year = {1992}
}
@article{Barroso2018,
author = {Barroso, Luiz Andr{\'{e}} and H{\"{o}}lzle, Urs and Ranganathan, Parthasarathy},
file = {:Volumes/GoogleDrive/My Drive/Thesis/Literature/barroso2018.pdf:pdf},
title = {{The Datacenter as Computer Third Edition}},
year = {2018}
}
@book{Tanenbaum2013,
address = {New Jersey},
author = {Tanenbaum, Andrew and Austin, Todd},
edition = {6},
editor = {Johnson, Tracy and Snyder, Carol and Holcomb, Jeff and Engelhardt, Robert and Sandin, Daniel},
file = {:Volumes/GoogleDrive/My Drive/Thesis/Literature/tanenbaum.pdf:pdf},
isbn = {978-0-13-291652-3},
keywords = {Computer Programming,Computer oraganization},
mendeley-tags = {Computer Programming,Computer oraganization},
pages = {776},
publisher = {Prentice Hall},
title = {{Structured Computer Organization}},
year = {2013}
}
@article{Vidal2019,
abstract = {Orchestration is crucial to address the growing complexity of IT, cloud and telecom services. Orchestrators enable service deployment, but continuous operation is given second-class status. Most orchestrators use static workflows, which may fail, misbehave or be insufficient, leaving services in an undesirable state. We propose the Reorchestration architecture for orchestration without workflows, which marries functional-inspired state management with an infrastructure-As-code approach. It enables changes to be applied to existing services, while providing a high-level framework for expressing how these changes should happen. We also share our first prototype, in the hope that it can foster discussions on new software architectures for orchestrators and closed-loop automation.},
author = {Vidal, Allan and Gomes, Pedro Henrique and Santos, Mateus},
doi = {10.1109/NETSOFT.2019.8806677},
file = {:Volumes/GoogleDrive/My Drive/Thesis/Literature/vidal2019.pdf:pdf},
isbn = {9781538693766},
journal = {Proceedings of the 2019 IEEE Conference on Network Softwarization: Unleashing the Power of Network Softwarization, NetSoft 2019},
keywords = {Orchestration,closed-loop automation,day-2 operations,workflows},
pages = {498--505},
publisher = {IEEE},
title = {{Reorchestration: A reactive orchestration architecture}},
year = {2019}
}
@article{Singh2013,
abstract = {The reliance on multi/many-core systems to satisfy the high performance requirement of complex embedded software applications is increasing. This necessitates the need to realize efficient mapping methodologies for such complex computing platforms. This paper provides an extensive survey and categorization of state-of-the-art mapping methodologies and highlights the emerging trends for multi/many-core systems. The methodologies aim at optimizing system's resource usage, performance, power consumption, temperature distribution and reliability for varying application models. The methodologies perform design-time and run-time optimization for static and dynamic workload scenarios, respectively. These optimizations are necessary to fulfill the end-user demands. Comparison of the methodologies based on their optimization aim has been provided. The trend followed by the methodologies and open research challenges have also been discussed. Copyright {\textcopyright} 2013 ACM.},
author = {Singh, Amit Kumar and Shafique, Muhammad and Kumar, Akash and Henkel, J{\"{o}}rg},
doi = {10.1145/2463209.2488734},
file = {:Volumes/GoogleDrive/My Drive/Thesis/Literature/singh2013.pdf:pdf},
isbn = {9781450320719},
issn = {0738100X},
journal = {Proceedings - Design Automation Conference},
keywords = {Application mapping,Embedded systems,Multiprocessor Systems-on-Chip},
title = {{Mapping on multi/many-core systems: Survey of current and emerging trends}},
year = {2013}
}
@article{Mokhtar2013,
abstract = {IT plays important role in education. Cloud Computing is a new technology in the world of IT which emerged recently and began to spread rapidly. Cloud computing is an Internet-based computing services on-demand. The main goal of this paper is to highlight the implications of cloud computing in academic institutions. This paper presents the cloud computing paradigm, historical view, concept, layers, types and the characteristics. During reviewing of many instances of using cloud computing in academic institutions, this paper emphasizes that the adoption of cloud computing in academic institutions is useful and sometimes necessary to overcome many of challenges and barriers. The presentation shows that the cloud computing technology enhances the performance in academic institutions as well as cloud computing plays significant role to reduction of costs. As the adoption of cloud computing in academic institutions is still low, this paper provides the factors that affect the adoption of cloud computing in the academic institutions. Successful transition to cloud computing in academic institutions depends on well definition for transition strategies. This paper provides suggestion for transition strategies into cloud computing in academic institutions. Copyright {\textcopyright} 2013 ACM.},
author = {Mokhtar, Shamsul Anuar and Ali, Siti Haryani Shaikh and Al-Sharafi, Abdulkarem and Aborujilah, Abdulaziz},
doi = {10.1145/2448556.2448558},
file = {:Volumes/GoogleDrive/My Drive/Thesis/Literature/mokhtar2013.pdf:pdf},
isbn = {9781450319584},
journal = {Proceedings of the 7th International Conference on Ubiquitous Information Management and Communication, ICUIMC 2013},
keywords = {Academic institutions,Cloud computing,Information technology},
title = {{Cloud computing in academic institutions}},
year = {2013}
}
@article{Kushida2015,
abstract = {Cloud computing is a revolution in computing architecture, transforming not only the “where” (location) of computing, but also the “how” (the manner in which software is produced and the tools available for the automation of business processes). Cloud computing emerged as we transitioned from an era in which underlying computing resources were both scarce and expensive to an era in which the same resources were cheap and abundant. There are many ways to implement cloud architectures, and most people are familiar with public cloud services such as Gmail or Facebook. However, much of the impact of cloud computing on the economy will be driven by how large enterprises implement cloud architectures. Cloud is also poised to disrupt the Information Technology (IT) industry, broadly conceived, with a new wave of commoditization. Offerings optimized for high performance in an era of computing resource scarcity are giving way to loosely coupled, elastically managed architectures making use of cheap, abundant computing resources today.},
author = {Kushida, Kenji E. and Murray, Jonathan and Zysman, John},
doi = {10.1007/s10842-014-0188-y},
file = {:Volumes/GoogleDrive/My Drive/Thesis/Literature/Kushida2015.pdf:pdf},
issn = {15737012},
journal = {Journal of Industry, Competition and Trade},
keywords = {Cloud computing,Commoditization,Disruptive technology,IT management},
number = {1},
pages = {5--19},
title = {{Cloud Computing: From Scarcity to Abundance}},
volume = {15},
year = {2015}
}
@article{Iosup2017,
abstract = {In the new Digital Economy, massive computer systems, often grouped in datacenters, serve as factories 'producing' cloud services with massive consumption. However, to afford cloud services globally, we must address new research challenges in designing, operating, and using modern datacenters. We must also address challenges in educating and training the next generation of datacenter engineers. Addressing such challenges, in this work we present our vision on OpenDC: we envision the exploration of various datacenter concepts and technologies, using existing and new scientific methods, enabling new education practices and topics, and leading to the creation of new software and data artifacts. We present the datacenter concepts and technologies we are currently planning to explore using OpenDC. We identify the scientific methods we want to use, and explain our vision of education practices. We present the architecture and open-source program underlying the OpenDC software, and the format and open-Access data we use for datacenter experiments. We conclude with an open invitation for the community to join our effort.},
author = {Iosup, Alexandru and Andreadis, Georgios and Beek, Vincent Van and Bijman, Matthijs and Eyk, Erwin Van and Neacsu, Mihai and Overweel, Leon and Talluri, Sacheendra and Versluis, Laurens and Visser, Maaike},
doi = {10.1109/ISPDC.2017.25},
file = {:Volumes/GoogleDrive/My Drive/Thesis/Literature/iosup2017.pdf:pdf},
isbn = {9781538608623},
journal = {Proceedings - 2017 IEEE 16th International Symposium on Parallel and Distributed Computing, ISPDC 2017},
keywords = {Big data,Cloud computing,Datacenter operation,Experimentation,Opendc,Scheduling,Simulation},
pages = {85--94},
title = {{The opendc vision: Towards collaborative datacenter simulation and exploration for everybody}},
year = {2017}
}
@article{Iosup2008,
abstract = {While large grids are currently supporting the work of thousands of scientists, very little is known about their actual use. Due to strict organizational permissions, there are few or no traces of grid workloads available to the grid researcher and practitioner. To address this problem, in this work we present the Grid Workloads Archive (GWA), which is at the same time a workload data exchange and a meeting point for the grid community. We define the requirements for building a workload archive, and describe the approach taken to meet these requirements with the GWA. We introduce a format for sharing grid workload information, and tools associated with this format. Using these tools, we collect and analyze data from nine well-known grid environments, with a total content of more than 2000 users submitting more than 7 million jobs over a period of over 13 operational years, and with working environments spanning over 130 sites comprising 10 000 resources. We show evidence that grid workloads are very different from those encountered in other large-scale environments, and in particular from the workloads of parallel production environments: they comprise almost exclusively single-node jobs, and jobs arrive in "bags-of-tasks". Finally, we present the immediate applications of the GWA and of its content in several critical grid research and practical areas: research in grid resource management, and grid design, operation, and maintenance. {\textcopyright} 2008 Elsevier B.V. All rights reserved.},
author = {Iosup, Alexandru and Li, Hui and Jan, Mathieu and Anoep, Shanny and Dumitrescu, Catalin and Wolters, Lex and Epema, Dick H.J.},
doi = {10.1016/j.future.2008.02.003},
file = {:Volumes/GoogleDrive/My Drive/Thesis/Literature/iosup2008.pdf:pdf},
issn = {0167739X},
journal = {Future Generation Computer Systems},
keywords = {Grid computing,Grid traces,Grid workloads},
number = {7},
pages = {672--686},
title = {{The Grid Workloads Archive}},
volume = {24},
year = {2008}
}
@article{Moreira2010,
abstract = {The authors argue that the minimum cost of computing can be provided by consolidating real-time workloads onto relatively large servers, which can operate at high utilization while maintaining required response time, and then filling the remaining overhead capacity with batch-like workloads sold at a significantly reduced price. {\textcopyright} 2006 IEEE.},
author = {Moreira, Jose E. and Karidis, John P.},
doi = {10.1109/MM.2010.74},
file = {:Volumes/GoogleDrive/My Drive/Thesis/Literature/moreira2010.pdf:pdf},
issn = {02721732},
journal = {IEEE Micro},
keywords = {and hardware,cost of computing,data center,energy-proportional computing,servers},
number = {4},
pages = {25--28},
title = {{The case for full-throttle computing: An alternative datacenter design strategy}},
volume = {30},
year = {2010}
}
@article{Barroso2009,
abstract = {& w w w . m o r g a n c l a y p o o l . c o m As computation continues to move into the cloud, the computing platform of interest no longer resembles a pizza box or a refrigerator, but a warehouse full of computers. These new large datacenters are quite different from traditional hosting facilities of earlier times and cannot be viewed simply as a collection of co-located servers. Large portions of the hardware and software resources in these facilities must work in concert to efficiently deliver good levels of Internet service performance, something that can only be achieved by a holistic approach to their design and deployment. In other words, we must treat the datacenter itself as one massive warehouse-scale computer (WSC). We describe the architecture of WSCs, the main factors influencing their design, operation, and cost structure, and the characteristics of their software base. We hope it will be useful to architects and programmers of today's WSCs, as well as those of future many-core platforms which may one day implement the equivalent of today's WSCs on a single board. NOTES FOR THE SECOND EDITION After nearly four years of substantial academic and industrial developments in warehouse-scale computing, we are delighted to present our first major update to this lecture. The increased popularity of public clouds has made WSC software techniques relevant to a larger pool of programmers since our first edition. Therefore, we expanded Chapter 2 to reflect our better understanding of WSC software systems and the toolbox of software techniques for WSC programming. In Chapter 3, we added to our coverage of the evolving landscape of wimpy vs. brawny server trade-offs, and we now present an overview of WSC interconnects and storage systems that was promised but lacking in the original edition. Thanks largely to the help of our new co-author, Google Distinguished Engineer Jimmy Clidaras, the material on facility mechanical and power distribution design has been updated and greatly extended (see Chapters 4 and 5). Chapters 6 and 7 have also been revamped significantly. We hope this revised edition continues to meet the needs of educators and professionals in this area.},
author = {Barroso, Luiz Andr{\'{e}} and H{\"{o}}lzle, Urs},
doi = {10.2200/S00193ED1V01Y200905CAC006},
file = {:Volumes/GoogleDrive/My Drive/Thesis/Literature/barroso2009.pdf:pdf},
isbn = {159829556X},
issn = {1935-3235},
journal = {Synthesis Lectures on Computer Architecture},
month = {jan},
number = {1},
pages = {1--108},
title = {{The Datacenter as a Computer: An Introduction to the Design of Warehouse-Scale Machines}},
url = {http://www.amazon.com/The-Datacenter-Computer-Introduction-Warehouse-Scale/dp/159829556X http://www.morganclaypool.com/doi/abs/10.2200/S00193ED1V01Y200905CAC006},
volume = {4},
year = {2009}
}
@article{Andreadis2018-2,
abstract = {Datacenters act as cloud-infrastructure to stakeholders across industry, government, and academia. To meet growing demand yet operate efficiently, datacenter operators employ increasingly more sophisticated scheduling systems, mechanisms, and policies. Although many scheduling techniques already exist, relatively little research has gone into the abstraction of the scheduling process itself, hampering design, tuning, and comparison of existing techniques. In this work, we propose a reference architecture for datacenter schedulers. The architecture follows five design principles: components with clearly distinct responsibilities, grouping of related components where possible, separation of mechanism from policy, scheduling as complex workflow, and hierarchical multi-scheduler structure. To demonstrate the validity of the reference architecture, we map to it state-of-the-art datacenter schedulers. We find scheduler-stages are commonly underspecified in peer-reviewed publications. Through trace-based simulation and real-world experiments, we show underspecification of scheduler-stages can lead to significant variations in performance.},
archivePrefix = {arXiv},
arxivId = {1808.04224},
author = {Andreadis, Georgios and Versluis, Laurens and Mastenbroek, Fabian and Iosup, Alexandru},
eprint = {1808.04224},
file = {:Volumes/GoogleDrive/My Drive/Thesis/Literature/andreadis2018_technical_report.pdf:pdf},
month = {aug},
number = {around},
title = {{A Reference Architecture for Datacenter Scheduling: Extended Technical Report}},
url = {http://arxiv.org/abs/1808.04224},
year = {2018}
}
@article{Couto2012,
abstract = {The network infrastructure plays an important role for datacenter applications. Therefore, datacenter network architectures are designed with three main goals: bandwidth, latency and reliability. This work focuses on the last goal and provides a comparative analysis of the topologies of prevalent datacenter architectures. Those architectures use a network based only on switches or a hybrid scheme of servers and switches to perform packet forwarding. We analyze failures of the main networking elements (link, server, and switch) to evaluate the tradeoffs of the different datacenter topologies. Considering only the network topology, our analysis provides a baseline study to the choice or design of a datacenter network with regard to reliability. Our results show that, as the number of failures increases, the considered hybrid topologies can substantially increase the path length, whereas servers on the switch-only topology tend to disconnect more quickly from the main network. {\textcopyright} 2012 IEEE.},
author = {Couto, Rodrigo S. and Campista, Miguel Elias M. and Costa, Luis Henrique M.K.},
doi = {10.1109/GLOCOM.2012.6503391},
file = {:Volumes/GoogleDrive/My Drive/Thesis/Literature/couto2012.pdf:pdf},
isbn = {9781467309219},
journal = {GLOBECOM - IEEE Global Telecommunications Conference},
pages = {1890--1895},
title = {{A reliability analysis of datacenter topologies}},
year = {2012}
}
@article{IBM2014,
author = {{IBM Global Technology Services}},
file = {:Volumes/GoogleDrive/My Drive/Thesis/Literature/mc_asset_2846167__sfw03008usen-02_SFW03008USEN.pdf:pdf},
number = {September},
title = {{Prefabricated modular data center—add data
center capacity where and when you need it}},
year = {2014}
}
@article{Torell2014,
abstract = {White Paper 164 Standardized, scalable, pre-assembled, and integrated data center facility power and cooling modules provide a "total cost of ownership" (TCO) savings of 30% compared to traditional, built-out data center power and cooling infrastructure. Avoiding overbuilt capacity and scaling the design over time contributes to a significant percentage of the overall savings. Executive summary TCO Analysis of a Traditional Data Center vs. a Scalable, Prefabricated Data Center Schneider Electric-Data Center Science Center Rev 1 2 Power and cooling systems available now are more modular, more standardized, and more efficient than those installed in the majority of data centers today. Whether upgrading an existing data center or building a new one, data center managers will minimize both capital and operating expenses by specifying physical infrastructure with the following attributes: • Standardized, pre-assembled, and integrated components • Modular infrastructure than can scale as the load increases over time • Efficient power and cooling components • Cooling design with integrated economizer mode • Pre-programmed controls White Paper 163, Prefabricated Power and Cooling Modules for Data Centers, describes how standardized, pre-assembled, and integrated modules (sometimes referred to as containers) save deployment time and upfront cost compared to the same electrical and mechanical infrastructure implemented in a "stick built" manner with custom engineering and considerable onsite work. However, significant additional savings can be achieved. The modular nature of prefabricated facility modules enables scaling and rightsizing to actual data center loads. This, in combination with current power and cooling distribution technologies, results in a TCO savings of nearly 30% over a traditional data center (27.2% capital cost and 31.6% operating cost). Figure 1 and Figure 2 illustrate a CAPEX difference of 27.2% and an OPEX difference of 31.6% between a data center with traditional infrastructure and operational practices, and a data center with prefabricated facility modules designed and implemented with best practices. The '"waterfall" charts break out how the 27.2% CAPEX savings and 31.6% OPEX savings are derived.},
author = {Torell, Wendy},
file = {:Volumes/GoogleDrive/My Drive/Thesis/Literature/WTOL-8NDS37_R1_EN.pdf:pdf},
keywords = {Data center, Datacenter, Power, Cooling, Physical infrastructure, Efficiency, UPS, CRAC, Carbon, Computer room, Server room, Schneider Electric, TCO, Container, Containerized, Facility module},
pages = {1--7},
title = {{TCO Analysis of a Traditional Data Center vs. a Scalable, Prefabricated Data Center Revision 1}}
}
@article{Torell2017,
abstract = {Data center systems or subsystems that are pre-assembled in a factory are often described with terms like prefabricated, containerized, modular, skid-based, pod-based, mobile, portable, self-contained, all-in-one, and more. There are, however, important distinc-tions between the various types of factory-built building blocks on the market. This paper proposes standard terminology for categorizing the types of prefabricated modular data centers, defines and compares their key attributes, and provides a frame-work for choosing the best approach(es) based on business requirements. 1. made up of at least one pre-engineered, factory-integrated, and pre-tested assembly of subsystems that have traditionally been installed separately onsite 2. mounted on a skid or in an enclosure While the benefits of this type of approach to data centers is well-documented (such as in White Paper 163, Containerized Power and Cooling Modules for Data Centers), the lack of a common language and classification system has resulted in confusion over the differences in attributes and applications between them. This white paper presents a framework for classifying the different types of prefabricated modular data centers, based on three attributes, so that ambiguity is eliminated. Definitions and categories are presented, along with guidance for choosing the right approach(es) for given business requirements. Figure 1 illustrates the three attributes that, together, define the majority of prefabricated modular data centers.},
author = {Torell, Wendy},
file = {:Volumes/GoogleDrive/My Drive/Thesis/Literature/WTOL-97GLP9_R2_EN.pdf:pdf},
journal = {APC White Paper},
keywords = {Data center,Efficienc,Modular,Prefab,container},
number = {2},
pages = {13},
title = {{Types of Prefabricated Modular Data Centers}},
volume = {165},
year = {2017}
}
@article{Wittern2016,
abstract = {The node package manager (npm) serves as the frontend to a large repository of JavaScript-based software packages, which foster the development of currently huge amounts of server-side Node.js and client-side JavaScript applications. In a span of 6 years since its inception, npm has grown to become one of the largest software ecosystems, hosting more than 230, 000 packages, with hundreds of millions of package installations every week. In this paper, we examine the npm ecosystem from two complementary perspectives: 1) we look at package descriptions, the dependencies among them, and download metrics, and 2) we look at the use of npm packages in publicly available applications hosted on GitHub. In both perspectives, we consider historical data, providing us with a unique view on the evolution of the ecosystem. We present analyses that provide insights into the ecosystem's growth and activity, into conflicting measures of package popularity, and into the adoption of package versions over time. These insights help understand the evolution of npm, design better package recommendation engines, and can help developers understand how their packages are being used.},
author = {Wittern, Erik and Suter, Philippe and Rajagopalan, Shriram},
doi = {10.1145/2901739.2901743},
file = {:Volumes/GoogleDrive/My Drive/Thesis/Literature/wittern2016.pdf:pdf},
isbn = {9781450341868},
journal = {Proceedings - 13th Working Conference on Mining Software Repositories, MSR 2016},
keywords = {JavaScript,Node.js,Software ecosystem analysis},
pages = {351--361},
title = {{A look at the dynamics of the JavaScript package ecosystem}},
year = {2016}
}
@article{Bang2020,
abstract = {In this paper, we set out the goal to revisit the results of "Starring into the Abyss [..] of Concurrency Control with [1000] Cores" [27] and analyse in-memory DBMSs on today's large hardware. Despite the original assumption of the authors, today we do not see single-socket CPUs with 1000 cores. Instead multi-socket hardware made its way into production data centres. Hence, we follow up on this prior work with an evaluation of the characteristics of concurrency control schemes on real production multi-socket hardware with 1568 cores. To our surprise, we made several interesting findings which we report on in this paper.},
author = {Bang, Tiemo and May, Norman and Petrov, Ilia and Binnig, Carsten},
doi = {10.1145/3399666.3399910},
file = {:Volumes/GoogleDrive/My Drive/Thesis/Literature/bang2020.pdf:pdf},
isbn = {9781450380249},
journal = {Proceedings of the 16th International Workshop on Data Management on New Hardware, DaMoN 2020},
title = {{The tale of 1000 Cores: An evaluation of concurrency control on real(ly) large multi-socket hardware}},
year = {2020}
}
@misc{SURF2020,
author = {SURF},
title = {{Description of the Lisa system}},
url = {https://userinfo.surfsara.nl/systems/lisa/description},
urldate = {2020-08-03},
year = {2020}
}


